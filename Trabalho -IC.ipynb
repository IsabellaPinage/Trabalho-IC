{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfddcdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Homework 5: Support Vector Machines (SVMs)\n",
    "\n",
    "# Aluno: Isabella Pinagé Soares\n",
    "# Curso: Inteligencia Computacional\n",
    "# Professor: Aldebaro Klautal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6af2edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy  \n",
    "%pip install matplotlib scikit-learn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de590c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Questão 1: Análise Visual de Regiões de Decisão\n",
    "# no conjunto LinearSVC tem dois erros; no conjunto SVC with linear kernel tem um erro;\n",
    "# no conjunto SVC with RBF kernel tem um erro;\n",
    "# no conjunto SVC with polynomial kernel não possui erro;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaf785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Questão 2:Regularização com parâmetro C\n",
    "#Deve-se diminuir o parametro C para aumentar o numero de vetores de suporte, pois o C controla o equilibrio entre minimizar o erro\n",
    "#de classificação e maximizar a margem. Assim, um valor menor de C resulta em uma margem maior, o que leva a um maior número de vetores de suporte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320241b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fator de redução de custo computacional: 450.00\n"
     ]
    }
   ],
   "source": [
    "#Questão 3: Fator de redução de custo computacional\n",
    "K = 5\n",
    "n_sv = 450\n",
    "C_original = n_sv * (K + (K - 1))  # 5 mult + 4 add por vetor de suporte\n",
    "C_perceptron = K + (K - 1)         # 5 mult + 4 add no perceptron\n",
    "F = C_original / C_perceptron\n",
    "print(f\"Fator de redução de custo computacional: {F:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ccebbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(z) = -2.00, y = 0\n",
      "Vetor w convertido: [-1.5 -6.9], bias: -2\n"
     ]
    }
   ],
   "source": [
    "# Questão 4: Interpretação dos coeficientes da SVM\n",
    "sv = np.array([[1, 4], [-2, 3], [-2, -5]]) # define os vetores de suporte\n",
    "lambdas = np.array([-0.5, -0.3, 0.8])# coeficientes que ponderam a contribuição de cada vetor de suporte\n",
    "bias = -2 #termo independente do hiperplano\n",
    "\n",
    "def decision_function(z):\n",
    "    return np.sum(lambdas * sv.dot(z)) + bias # formula geral da SVM (faz o produto escalar e multiplica pelo lambda e osma com bias)\n",
    "\n",
    "z = np.array([0, 0])  # Testando a função de decisão com um ponto z=(0,0)\n",
    "fz = decision_function(z)\n",
    "y_pred = int(fz > 0)\n",
    "print(f\"f(z) = {fz:.2f}, y = {y_pred}\")\n",
    "# Conversão para perceptron\n",
    "w = (-0.5 * sv[0]) + (-0.3 * sv[1]) + (0.8 * sv[2])\n",
    "print(f\"Vetor w convertido: {w}, bias: {bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a907e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia no teste: 1.00\n",
      "Acurácia no teste: 1.00\n",
      "Número de SVs: 4\n",
      "Custo SVM original: 12 operações\n",
      "Custo perceptron: 3 operações\n",
      "Fator de ganho computacional: 4.00x\n",
      "Vetor w convertido: [-1.5 -6.9], bias: -2\n"
     ]
    }
   ],
   "source": [
    "# Questão 6: treino de uma SVM com C=1\n",
    "train_data = np.loadtxt('dataset_train.txt', delimiter=',')\n",
    "test_data = np.loadtxt('dataset_test.txt', delimiter=',')\n",
    "val_data = np.loadtxt('dataset_validation.txt', delimiter=',')\n",
    "\n",
    "X_train, y_train = train_data[:, :-1], train_data[:, -1] #[:, :-1]: Pega todas as colunas menos a ultima\n",
    "X_test, y_test = test_data[:, :-1], test_data[:, -1] #[:, -1] Pega só a ultima coluna\n",
    "X_val, y_val = val_data[:, :-1], val_data[:, -1]\n",
    "\n",
    "# Normalização (baseada no conjunto de treino)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "X_val_norm = scaler.transform(X_val)\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1) #SVM linear com C=1, que busca um equilíbrio entre margem ampla e poucos erros\n",
    "\n",
    "clf.fit(X_train_norm, y_train) # Treinar o modelo com os dados normalizados de treino\n",
    "\n",
    "\n",
    "y_pred_test = clf.predict(X_test_norm) # modelo treinado para prever os rótulos do conjunto de teste\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test) # Avaliar a acurácia da predição comparando com os rótulos reais\n",
    "print(f\"Acurácia no teste: {test_accuracy:.2f}\")\n",
    "\n",
    "# Conversão da SVM linear para perceptron: Extrair o vetor de pesos w e o bias b diretamente do modelo treinado\n",
    "w = clf.coef_[0]  # vetor de pesos\n",
    "b = clf.intercept_[0]  # bias\n",
    "\n",
    "def perceptron_decision(x):  #função que simula o perceptron usando a forma compacta f(z) = <w, z> + b\n",
    "    return np.dot(w, x) + b\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "clf.fit(X_train_norm, y_train)\n",
    "y_pred_test = clf.predict(X_test_norm)\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Acurácia no teste: {acc_test:.2f}\")\n",
    "\n",
    "# Conversão para perceptron\n",
    "w = clf.coef_[0]\n",
    "b = clf.intercept_[0]\n",
    "def perceptron_decision(x):\n",
    "    return np.dot(w, x) + b\n",
    "\n",
    "# Estimar o custo computacional da fase de teste para comparar as formas:\n",
    "# Forma original da SVM: envolve todos os vetores de suporte (n_sv)\n",
    "# Forma perceptron: usar apenas o vetor w e bias b extraídos\n",
    "\n",
    "n_sv = len(clf.support_)\n",
    "K = X_test.shape[1]  # número de features\n",
    "\n",
    "# Custo da SVM original: para cada exemplo, n_sv produtos internos (cada um com K mult + K-1 somas)\n",
    "C_svm = n_sv * (K + (K - 1))\n",
    "\n",
    "# Custo do perceptron: um único produto interno \n",
    "C_perceptron = K + (K - 1)\n",
    "\n",
    "# Fator de ganho\n",
    "F = C_svm / C_perceptron\n",
    "print(f\"Número de SVs: {n_sv}\")\n",
    "print(f\"Custo SVM original: {C_svm} operações\")\n",
    "print(f\"Custo perceptron: {C_perceptron} operações\")\n",
    "print(f\"Fator de ganho computacional: {F:.2f}x\")\n",
    "\n",
    "#perceptron precisa armazenar apenas w e b, enquanto a SVM armazena todos os SVs e os lambdas\n",
    "w = (-0.5 * sv[0]) + (-0.3 * sv[1]) + (0.8 * sv[2])\n",
    "print(f\"Vetor w convertido: {w}, bias: {bias}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
